{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/dl_uwr/blob/summer2020/Assignment2/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXgWugfJ0Vl",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Submission deadline: last lab session before or on Thursday, 26.03.2020**\n",
        "\n",
        "**Points: 6 + 1 bonus points**\n",
        "\n",
        "## Submission instructions\n",
        "The class is held remotely. To sumbmit your solutions please save the notebook to your Google Drive, then:\n",
        "1. Rename it it to: Assignment2_Surname_FirstName\n",
        "2. Rerun the whole notebook `Runtime -> Restar and run all`\n",
        "3. Make a pinned revision `File->Save and pin revision`\n",
        "4. Share the notebook with your instructor using his `cs.uni.wroc.pl` email\n",
        "\n",
        "We will use the commenting system and video conferences to check and discuss the solutions.\n",
        "\n",
        "As always, please submit corrections using GitHub's Pull Requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfVDe-bMqVT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiTEWD2oqW0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZCM_hdELE04",
        "colab_type": "text"
      },
      "source": [
        "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
        "\n",
        "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
        "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
        "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
        "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D` matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEbCfbSpv5M",
        "colab_type": "code",
        "outputId": "f00ddeb4-b61c-4e0d-d1d4-f7fdfca62930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# Let's define a XOR dataset\n",
        "\n",
        "# X will be matrix of N 2-dimensional inputs\n",
        "X = np.array(\n",
        "    [[0, 0],\n",
        "     [0, 1],\n",
        "     [1, 0],\n",
        "     [1, 1],\n",
        "    ], dtype=np.float32)\n",
        "# Y is a matrix of N numners - answers\n",
        "Y = np.array(\n",
        "    [[0],\n",
        "     [1],\n",
        "     [1],\n",
        "     [0],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=Y[:,0], )\n",
        "plt.xlabel('X[0]')\n",
        "plt.ylabel('X[1]')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'X[1]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb3azMn929_I",
        "colab_type": "text"
      },
      "source": [
        "# Problem 1 [2p]\n",
        "\n",
        "Fill in the details of a forward pass, then manually set the weights and biases in the network to solve the 2D XOR task defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrrRuk6zLiF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return TODO\n",
        "\n",
        "class SmallNet:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        # TODO Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        pass\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "\n",
        "        # Input to neurons in 1st layer\n",
        "        A1 = TODO\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O1 = sigmoid(A1)\n",
        "        # Inputs to neuron in the second layer\n",
        "        A2 = TODO\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O2 = TODO\n",
        "\n",
        "        if Y is not None:\n",
        "            loss = TODO cross-entropy loss\n",
        "            # normalize loss by batch size\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            # TODO in Problem 2: fill in the gradient computation\n",
        "            # Please note, thate there is a correspondance between\n",
        "            # the forward and backward pass: with backward computations happening\n",
        "            # in reversed order. \n",
        "\n",
        "            # A2_grad is the gradient of loss with respect to A2\n",
        "            # Hint: there is a concise formula for the gradient \n",
        "            # of logistic sigmoid and cross-entropy loss \n",
        "            A2_grad: O2 = TODO\n",
        "            self.b2_grad = A2_grad.sum(0)\n",
        "            self.W2_grad: O2 = TODO\n",
        "            O1_grad: O2 = TODO\n",
        "            A1_grad: O2 = TODO\n",
        "            self.b1_grad: O2 = TODO\n",
        "            self.W1_grad: O2 = TODO\n",
        "\n",
        "        return O2, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJswvBk0oiIY",
        "colab_type": "code",
        "outputId": "331c7ce6-a3ad-4121-f6e1-d09d5ab6bcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# TODO Problem 1:\n",
        "# Set by hand the weight values to solve the XOR problem\n",
        "\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "net.W1 = TODO\n",
        "net.b1 = TODO\n",
        "net.W2 = TODO\n",
        "net.b2 = TODO\n",
        "\n",
        "# Hint: since we use the logistic sigmoid activation, the weights may need to \n",
        "# be fairly large \n",
        "\n",
        "\n",
        "net.forward(X, Y, do_backward=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.10683622],\n",
              "        [0.85086711],\n",
              "        [0.85086711],\n",
              "        [0.10683622]]), 0.13724231150961305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmxCi5Vl6_xB",
        "colab_type": "text"
      },
      "source": [
        "# Problem 2 [2p]\n",
        "\n",
        "1. Fill in the backward pass.\n",
        "2. Implement random initialization of network parameters.\n",
        "3. Using `float64` verify correctness of your backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSM5hgJ1mrhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
        "    \"\"\"A gradient checking routine\"\"\"\n",
        "    \n",
        "    param = getattr(net, param_name)\n",
        "    param_flat_accessor = param.reshape(-1)\n",
        "\n",
        "    grad = np.empty_like(param)\n",
        "    grad_flat_accessor = grad.reshape(-1)\n",
        "\n",
        "    net.forward(X, Y, do_backward=True)\n",
        "    orig_grad = getattr(net, param_name + '_grad')\n",
        "    assert (param.shape == orig_grad.shape)\n",
        "\n",
        "    for i in range(param_flat_accessor.shape[0]):\n",
        "        orig_val = param_flat_accessor[i]\n",
        "        param_flat_accessor[i] = orig_val + eps\n",
        "        _, loss_positive = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val - eps\n",
        "        _, loss_negative = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val\n",
        "        grad_flat_accessor[i] = (\n",
        "            loss_positive - loss_negative) / (2 * eps)\n",
        "    assert np.allclose(grad, orig_grad)\n",
        "    return grad, orig_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTZu0jFEvgXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "\n",
        "for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "    check_grad(net, param_name, X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUOs3cVvjM2",
        "colab_type": "text"
      },
      "source": [
        "# Problem 3 [2p]\n",
        "\n",
        "Fill in the details of batch gradient descent below, then train a network to solve 2D XOR problem.\n",
        "\n",
        "Then test the reliability of solving the 3D XOR task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn2AAoZo0vjU",
        "colab_type": "code",
        "outputId": "88d897f3-e3ca-47ea-92c5-878964344f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "net = SmallNet(2, 10, dtype=np.float64)\n",
        "\n",
        "alpha = 1e-1\n",
        "\n",
        "for i in range(100000):\n",
        "    _, loss = net.forward(X, Y, do_backward=True)\n",
        "    if (i % 5000) == 0:\n",
        "        print(i, loss)\n",
        "    for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "        param = getattr(net, param_name)\n",
        "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "        # Doing instead `param = new_val` simply changes to what the variable\n",
        "        # param points to, without affecting the network!\n",
        "        param[:] = TODO"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7956506013236352\n",
            "5000 0.03552764455685801\n",
            "10000 0.009967265977656918\n",
            "15000 0.005506227955558411\n",
            "20000 0.003740473680287438\n",
            "25000 0.0028088029700036087\n",
            "30000 0.0022375911750205655\n",
            "35000 0.0018533297673820938\n",
            "40000 0.0015779828744044225\n",
            "45000 0.0013714359113760873\n",
            "50000 0.0012110222315038753\n",
            "55000 0.0010829958306159686\n",
            "60000 0.000978549699353968\n",
            "65000 0.0008917875382479552\n",
            "70000 0.0008186169907951203\n",
            "75000 0.0007561110749308059\n",
            "80000 0.0007021223231953176\n",
            "85000 0.0006550403634071096\n",
            "90000 0.0006136344609745337\n",
            "95000 0.0005769482579549537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwpEjpkU1JvK",
        "colab_type": "code",
        "outputId": "8bcbfa0e-5845-4c53-cf66-3010eec03a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net.forward(X, Y, do_backward=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[3.26689329e-04],\n",
              "        [9.99506827e-01],\n",
              "        [9.99375126e-01],\n",
              "        [7.31535844e-04]]), 0.0005442276126349717)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3NiL0ku2Unr",
        "colab_type": "text"
      },
      "source": [
        "Generate below data for a 3D XOR task. Try a few values of hidden layer size. Plot the reliability of training, i.e. how many trainings succeed for a given hiden layer size.\n",
        "\n",
        "What is easier to train: a smaller, or large network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0ZMyHqz8xrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X3 = TODO\n",
        "Y3 = TODO\n",
        "\n",
        "for hidden_dim in [2, 3, 5, 10, 20]:\n",
        "    # TODO: run a few trainings and record the fraction of successful ones"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaLEoV-9DLG",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4 [1bp]\n",
        "\n",
        "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Verify ho"
      ]
    }
  ]
}